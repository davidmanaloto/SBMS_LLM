Ollama can be used to run a server.
It provides an API acting as a local server with the port 11434 as dafault.
Use http://localhost:11434/api/generate in Postman for testing.

Note: in my time testing, LLMs struggle with generating or fetching time&date. It cannot generate a calendar schedule nor provide one.  

About LLMs:
    You don't need a powerful computer to run an LLM. (8gb of ram required for the smallest one)

    LLMs vary in speed when generating if we are talking about hardware, their intelligence remains the same.

    LLMs really likes using the architecture of GPUs, I dont know the difference in speed but a 2gb model running on CPU has a response time of 32 seconds with a small dataset and summary promt.

    LLMs vary in sizes, intelligences, and context window.

    Larger models have more human like responses but needs powerful computers for faster generation.

    Smaller models have limited vocabulary and context window, it is faster but more restrict when generating results.

